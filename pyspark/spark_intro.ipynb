{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_unixtime,\n",
    "    to_timestamp,\n",
    "    min,\n",
    "    max,\n",
    "    sum,\n",
    "    avg,\n",
    "    col,\n",
    "    countDistinct,\n",
    "    broadcast,\n",
    "    date_trunc,\n",
    "    count,\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\"./iot_malware/CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv\", \"./iot_malware/CTU-IoT-Malware-Capture-3-1conn.log.labeled.csv\"]\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"iot\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \"|\").csv(filepaths, inferSchema = True, header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dt\", from_unixtime(\"ts\")).withColumn(\"dt\", to_timestamp(\"dt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnsRenamed(\n",
    "    {\n",
    "        \"id.orig_h\": \"source_ip\",\n",
    "        \"id.orig_p\": \"source_port\",\n",
    "        \"id.resp_h\": \"dest_ip\",\n",
    "        \"id.resp_p\": \"dest_port\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min, Max datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(\n",
    "    min(\"dt\").alias(\"min_date\"), \n",
    "    max(\"dt\").alias(\"max_date\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyse = [\n",
    "    \"source_ip\",\n",
    "    \"source_port\",\n",
    "    \"dest_ip\",\n",
    "    \"dest_port\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]\n",
    "\n",
    "unique_counts = df.agg(*(countDistinct(col(c)).alias(c) for c in to_analyse))\n",
    "print(unique_counts.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = unique_counts.first()\n",
    "static_cols = [c for c in unique_counts.asDict() if unique_counts[c] == 1]\n",
    "print(\"Dataset has\", len(static_cols), \"static columns: \", static_cols)\n",
    "df = df.drop(*static_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ips = df.select(col(\"source_ip\")).distinct()\n",
    "dest_ips = df.select(col(\"dest_ip\")).distinct()\n",
    "common_ips = source_ips.join(broadcast(dest_ips), source_ips.source_ip == dest_ips.dest_ip, how='inner')\n",
    "\n",
    "\n",
    "print(\"Source IPs count:\", source_ips.count())\n",
    "print(\"Dest IPs count:\", dest_ips.count())\n",
    "print(\"IPs as both:\", common_ips.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ports = df.select(col(\"source_port\")).distinct()\n",
    "dest_ports = df.select(col(\"dest_port\")).distinct()\n",
    "common_ports = source_ports.join(broadcast(dest_ports), source_ports.source_port == dest_ports.dest_port, how='inner')\n",
    "\n",
    "\n",
    "print(\"Source Ports count:\", source_ports.count())\n",
    "print(\"Dest Ports count:\", dest_ports.count())\n",
    "print(\"Ports as both:\", common_ports.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(\"-\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_cols = [f for f in to_analyse if f not in static_cols]\n",
    "df.select(\n",
    "    [count(F.when(F.isnan(c) | col(c).isNull(), c)).alias(c) for c in remaining_cols]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumns(\n",
    "    {\n",
    "        \"day\": date_trunc(\"day\", \"dt\"),\n",
    "        \"hour\": date_trunc(\"hour\", \"dt\"),\n",
    "        \"minute\": date_trunc(\"minute\", \"dt\"),\n",
    "        \"second\": date_trunc(\"second\", \"dt\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agg in ['day', 'hour', 'minute']:\n",
    "    plotting_table = df.groupBy([agg, \"label\"]).agg(count(\"uid\").alias(\"counts\")).orderBy(agg).toPandas()\n",
    "    fig = px.line(plotting_table, x=agg, y=\"counts\", color=\"label\", title=f'Event Counts per {agg}')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts(df, var):\n",
    "    var_counts = df.groupBy(var).count().orderBy(\"count\", ascending=False)\n",
    "    var_counts = var_counts.withColumn(\n",
    "        \"percent\", F.round(col(\"count\") / sum(col(\"count\")).over(Window.partitionBy()), 4)\n",
    "    )\n",
    "    var_counts.show()\n",
    "    fig = px.bar(var_counts.toPandas(), x=var, y=\"count\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "categorical_columns = [\"proto\", \"service\", \"conn_state\", \"history\", \"label\"]\n",
    "\n",
    "for c in categorical_columns:\n",
    "    counts(df, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "]\n",
    "categorical_cols = [\"proto\", \"service\", \"conn_state\"]\n",
    "label = \"label\"\n",
    "\n",
    "all_cols = numerical_cols + categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recast_cols = {}\n",
    "fill_vals = {}\n",
    "for c in numerical_cols:\n",
    "    recast_cols[c] = col(c).cast(\"double\")\n",
    "    fill_vals[c] = -999999\n",
    "\n",
    "for c in categorical_cols:\n",
    "    fill_vals[c] = 'missing'\n",
    "    \n",
    "df = df.withColumns(recast_cols)\n",
    "df = df.fillna(fill_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = [\"local_orig\", \"local_resp\", \"missed_bytes\", \"tunnel_parents\"]\n",
    "\n",
    "recast_cols = {\n",
    "    \"duration\": col(\"duration\").cast(\"double\"),\n",
    "    \"orig_bytes\": col(\"orig_bytes\").cast(\"double\"),\n",
    "    \"resp_bytes\": col(\"resp_bytes\").cast(\"double\"),\n",
    "    \"orig_ip_bytes\": col(\"orig_ip_bytes\").cast(\"double\"),\n",
    "    \"orig_pkts\": col(\"orig_pkts\").cast(\"double\"),\n",
    "    \"resp_pkts\": col(\"resp_pkts\").cast(\"double\"),\n",
    "    \"resp_ip_bytes\": col(\"resp_ip_bytes\").cast(\"double\"),\n",
    "}\n",
    "\n",
    "fill_vals = {\n",
    "    \"duration\": -999999,\n",
    "    \"orig_bytes\": -999999,\n",
    "    \"resp_bytes\": -999999,\n",
    "    \"orig_pkts\": -999999,\n",
    "    \"orig_ip_bytes\": -999999,\n",
    "    \"resp_pkts\": -999999,\n",
    "    \"resp_ip_bytes\": -999999,\n",
    "    \"history\": \"missing\",\n",
    "    \"proto\": \"missing\",\n",
    "    \"service\": \"missing\",\n",
    "    \"conn_state\": \"missing\",\n",
    "}\n",
    "\n",
    "preprocessed_data = (\n",
    "    spark.read.option(\"delimiter\", \"|\")\n",
    "    .csv(filepaths, inferSchema=True, header=True)\n",
    "    .withColumn(\"dt\", to_timestamp(from_unixtime(\"ts\")))\n",
    "    .withColumns(\n",
    "        {\n",
    "            \"day\": date_trunc(\"day\", \"dt\"),\n",
    "            \"hour\": date_trunc(\"hour\", \"dt\"),\n",
    "            \"minute\": date_trunc(\"minute\", \"dt\"),\n",
    "            \"second\": date_trunc(\"second\", \"dt\"),\n",
    "        }\n",
    "    )\n",
    "    .withColumnsRenamed(\n",
    "        {\n",
    "            \"id.orig_h\": \"source_ip\",\n",
    "            \"id.orig_p\": \"source_port\",\n",
    "            \"id.resp_h\": \"dest_ip\",\n",
    "            \"id.resp_p\": \"dest_port\",\n",
    "        }\n",
    "    )\n",
    "    .drop(*static_cols)\n",
    "    .replace(\"-\", None)\n",
    "    .withColumns(recast_cols)\n",
    "    .fillna(fill_vals)\n",
    ")\n",
    "\n",
    "preprocessed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.writeparquet(\"processed.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in = spark.read.parquet(\"processed.pq\")\n",
    "read_in.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
