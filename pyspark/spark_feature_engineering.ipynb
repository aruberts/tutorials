{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Project Step-by-Step: Part 2\n",
    "\n",
    "This notebook will walk you through 2 more steps in the ML lifecycle - **Feature Engineering** and **Model Fitting & Evaluation**.<br>\n",
    "* In the feature engineering part you'll see how to perform common aggregates using analytical functions.\n",
    "* In the modelling part you'll see how to prepare your data for modelling in PySpark, and how to fit a model using MLLib.\n",
    "* Finally, we'll see how we can evaluate the model we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"iot\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"processed.pq\").withColumn(\n",
    "    \"is_bad\", F.when(F.col(\"label\") != \"Benign\", 1).otherwise(0)\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Since we have a time-component to this data, we can engineer all sorts of rolling features. The ones that I'll cover here are:\n",
    "* Number of times we've seen this source IP in the last minute\n",
    "* Number of times we've seen this destination IP in the last minute\n",
    "* Number of times we've seen this source PORT in the last minute\n",
    "* Number of times we've seen this destination PORT in the last minute\n",
    "\n",
    "To calculate these features, we'll need to use analytical functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mins_to_secs(mins):\n",
    "    return mins * 60\n",
    "\n",
    "\n",
    "def generate_window(window_in_minutes: int, partition_by: str, timestamp_col: str):\n",
    "    window = (\n",
    "        Window()\n",
    "        .partitionBy(F.col(partition_by))\n",
    "        .orderBy(F.col(timestamp_col).cast(\"long\"))\n",
    "        .rangeBetween(-mins_to_secs(window_in_minutes), -1)\n",
    "    )\n",
    "\n",
    "    return window\n",
    "\n",
    "\n",
    "def generate_rolling_aggregate(\n",
    "    col: str,\n",
    "    partition_by: str | None = None,\n",
    "    operation: str = \"count\",\n",
    "    timestamp_col: str = \"dt\",\n",
    "    window_in_minutes: int = 1,\n",
    "):\n",
    "    if partition_by is None:\n",
    "        partition_by = col\n",
    "\n",
    "    match operation:\n",
    "        case \"count\":\n",
    "            return F.count(col).over(\n",
    "                generate_window(\n",
    "                    window_in_minutes=window_in_minutes,\n",
    "                    partition_by=col,\n",
    "                    timestamp_col=timestamp_col,\n",
    "                )\n",
    "            )\n",
    "        case \"sum\":\n",
    "            return F.sum(col).over(\n",
    "                generate_window(\n",
    "                    window_in_minutes=window_in_minutes,\n",
    "                    partition_by=col,\n",
    "                    timestamp_col=timestamp_col,\n",
    "                )\n",
    "            )\n",
    "        case \"avg\":\n",
    "            return F.avg(col).over(\n",
    "                generate_window(\n",
    "                    window_in_minutes=window_in_minutes,\n",
    "                    partition_by=col,\n",
    "                    timestamp_col=timestamp_col,\n",
    "                )\n",
    "            )\n",
    "        case _:\n",
    "            raise ValueError(f\"Operation {operation} is not defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Rolling Count Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nicely defined functions above, generating rolling averages and counts is a piece of cake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumns({\n",
    "    \"source_ip_count_last_min\": generate_rolling_aggregate(col=\"source_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_count_last_30_mins\": generate_rolling_aggregate(col=\"source_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_port_count_last_min\": generate_rolling_aggregate(col=\"source_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_port_count_last_30_mins\": generate_rolling_aggregate(col=\"source_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"dest_ip_count_last_min\": generate_rolling_aggregate(col=\"dest_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"dest_ip_count_last_30_mins\": generate_rolling_aggregate(col=\"dest_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"dest_port_count_last_min\": generate_rolling_aggregate(col=\"dest_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"dest_port_count_last_30_mins\": generate_rolling_aggregate(col=\"dest_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_ip_avg_pkts_last_min\": generate_rolling_aggregate(col=\"orig_pkts\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_avg_pkts_last_30_mins\": generate_rolling_aggregate(col=\"orig_pkts\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_ip_avg_bytes_last_min\": generate_rolling_aggregate(col=\"orig_ip_bytes\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_avg_bytes_last_30_mins\": generate_rolling_aggregate(col=\"orig_ip_bytes\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,execute and save the resulting table into a new parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"feature_engineered.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = spark.read.parquet(\"feature_engineered.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the speed of calling the old `df` vs the new `df_fe`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a drastic difference is because when you call `df.show()` it's going to execute all of the very expensive operations we did. Instead, it's better to construct a new dataframe for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"source_ip_count_last_min\",\n",
    "    \"source_ip_count_last_30_mins\",\n",
    "    \"source_port_count_last_min\",\n",
    "    \"source_port_count_last_30_mins\",\n",
    "    # \"dest_ip_count_last_min\",\n",
    "    # \"dest_ip_count_last_30_mins\",\n",
    "    # \"dest_port_count_last_min\",\n",
    "    # \"dest_port_count_last_30_mins\",\n",
    "    \"source_ip_avg_pkts_last_min\",\n",
    "    \"source_ip_avg_pkts_last_30_mins\",\n",
    "    \"source_ip_avg_bytes_last_min\",\n",
    "    \"source_ip_avg_bytes_last_30_mins\",\n",
    "]\n",
    "categorical_features = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "categorical_features_indexed = [c + \"_index\" for c in categorical_features]\n",
    "\n",
    "input_features = numerical_features + categorical_features_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select([F.count_distinct(c) for c in categorical_features]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_valid_values = {}\n",
    "\n",
    "for c in categorical_features:\n",
    "    # Find frequent values\n",
    "    categorical_valid_values[c] = (\n",
    "        df_fe.groupby(c)\n",
    "        .count()\n",
    "        .filter(F.col(\"count\") > 100)\n",
    "        .select(c)\n",
    "        .toPandas()\n",
    "        .values.ravel()\n",
    "    )\n",
    "\n",
    "    df_fe = df_fe.withColumn(\n",
    "        c,\n",
    "        F.when(F.col(c).isin(list(categorical_valid_values[c])), F.col(c)).otherwise(\n",
    "            F.lit(\"Other\").alias(c)\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select([F.count_distinct(c) for c in categorical_features]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Train test split will need to be done using the source IP address, otherwise we risk leaking data. The best way to do this is by splitting the IP addresses at random, and then filtering the data frame according to the IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.groupby(\"source_ip\").agg(F.sum(F.col(\"is_bad\")).alias(\"bad_sum\")).orderBy(\"bad_sum\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training non-malicious IPs (80%)\n",
    "train_ips = (\n",
    "    df_fe.where(\n",
    "        ~F.col(\"source_ip\").isin([\"192.168.100.103\", \"192.168.2.5\", \"192.168.2.1\"])\n",
    "    )\n",
    "    .select(F.col(\"source_ip\"), F.lit(1).alias(\"is_train\"))\n",
    "    .dropDuplicates()\n",
    "    .sample(0.8)\n",
    ")\n",
    "\n",
    "\n",
    "df_fe = df_fe.join(train_ips, \"source_ip\", \"left\")\n",
    "\n",
    "# Add 1 malicious IP to training and testing data\n",
    "df_train = df_fe.where((F.col(\"is_train\") == 1) | (F.col(\"source_ip\") == \"192.168.100.103\"))\n",
    "df_test = df_fe.where((F.col(\"is_train\") != 1) | (F.col(\"source_ip\") == \"192.168.2.5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = StringIndexer(inputCols=categorical_features, outputCols=categorical_features_indexed, handleInvalid='skip')\n",
    "va = VectorAssembler(inputCols=input_features, outputCol=\"features\", handleInvalid='skip' )\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_bad\", numTrees=100)\n",
    "\n",
    "pipeline = Pipeline(stages=[ind, va, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(df_train)\n",
    "test_preds = pipeline.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "roc = BinaryClassificationEvaluator(labelCol=\"is_bad\", metricName=\"areaUnderROC\")\n",
    "print(\"ROC AUC\", roc.evaluate(test_preds))\n",
    "\n",
    "pr = BinaryClassificationEvaluator(labelCol=\"is_bad\", metricName=\"areaUnderPR\")\n",
    "print(\"PR AUC\", pr.evaluate(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"importance\": list(pipeline.stages[-1].featureImportances),\n",
    "        \"feature\": pipeline.stages[-2].getInputCols(),\n",
    "    }\n",
    ").sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.stages[-1].save(\"rf_basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save(\"pipeline_basic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
